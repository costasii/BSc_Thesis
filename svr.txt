SUPPORT VECTOR REGRESSION
=========================
The Support Vector algorithm was developed in its present form at AT&T Bell Laboratories by Vapnik with colleagues. [11,12]

The Support Vector Machines are supervised learning models, that can be applied to classification, regression, and outliers detection problems. [13]

In classification problems, the output is a discrete variable, and in regression it is continuous.

The SV algorithm needs to find a function in n-dimensional space which fits the data. 

Support Vectors are the data points around this function. A flexible tube is formed symmetrically around the function so that the absolute values of errors less than a certain threshold are ignored both above and below the estimate. This way, the data points outside of the tube are penalized, and those on the inside are not. [14]

The SVR tries to find the narrowest tube centered around the function while minimizing the prediction error. [15]

The advantage of SVR is that its complexity does not depend on the dimension of the input space. [14]

Depending on the choice of kernel, data is mapped into different dimensional spaces.

Kernel functions can be linear, rbf, polynomial, and sigmoid.

The free parameters of the model are C and epsilon.



The model is defined by its hyperparameters. They control the way the model fits the data.

Their fine-tuning was performed using Bayesian optimization, which is based on the Bayesian theorem. More commonly used Grid Search tries out all given combinations of parameters. Bayes Search, as opposed to Grid Search, memorizes the result of the previous combination and uses it to choose the next combination, so that it comes to the optimum more quickly. [16]

The SVR, as well as other Support Vector models that will be mentioned, are implemented using scikit learn library. Implementation details are described in Chapter 5. Experiments and Results.


word2vec
========
Word2vec is a way to represent natural language as vectors. It was developed by Mikolov et al. in 2013. [17]
It consists of two learning models: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts the word given its context, and Skip-gram predicts the context given a word. [18]
Knowing the distances between vectors, it is possible to group similar words.
This algorithm is used to convert the text input into vectors, after which they can be used in Support Vector Regression tasks.


LinearSVR
=========

LinearSVR is similar to a Support Vector Regression model with a linear kernel. 
The difference is that LinearSVR is implemented as a liblinear model, and SVR as libsvm. 
It is advised to use LinearSVR over SVR for large datasets. [scikit learn]

SGDRegressor
============
SGD stands for Stochastic Gradient Descent. This is an iterative method of finding a function optimum. The gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing learning rate. [scikit learn]


















[11] - Boser et al. 1992. work
[12] - tutorial on support vector regression
[13] - scikit learn SVM
[14] - SVR book, downloaded bib
[15] - SVR book, chapter 1 
[16] - Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization, skinut bib
[18] - word2vec cited as unknown in bib
[17] - T. Mikolov, K. Chen, G. Corrado, J. Dean, “Efficient Estimation of
Word Representations in Vector Space,” Proc. Workshop at ICLR,
2013. 