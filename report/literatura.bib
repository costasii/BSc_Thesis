@article{cremer,
	author = {Cremer, F., Sheehan, B., Fortmann, M. et al.},
	year = {2022},
	title = {Cyber risk and cybersecurity: a systematic review of data availability},
	publisher = {Geneva Pap Risk Insur Issues Pract},
	url = {https://doi.org/10.1057/s41288-022-00266-6}
}

@article{humayun,
	author = {Humayun, Mamoona and Niazi, Mahmood and Zaman, Noor and Alshayeb, Mohammad and Mahmood, Sajjad},
	year = {2020},
	month = {01},
	pages = {},
	title = {Cyber Security Threats and Vulnerabilities: A Systematic Mapping Study},
	volume = {45},
	journal = {Arabian Journal for Science and Engineering},
	doi = {10.1007/s13369-019-04319-2}
}

@article{khazaei,
	author = {Khazaei, Atefeh and Ghasemzadeh, Mohammad and Derhami, Vali},
	year = {2015},
	month = {08},
	pages = {89-96},
	title = {An automatic method for CVSS score prediction using vulnerabilities description},
	volume = {30},
	journal = {Journal of Intelligent and Fuzzy Systems},
	doi = {10.3233/IFS-151733}
}

@unknown{ma,
	author = {Ma, Long and Zhang, Yanqing},
	year = {2015},
	month = {10},
	pages = {},
	title = {Using Word2Vec to process big text data},
	doi = {10.1109/BigData.2015.7364114}
}

@inproceedings{47751,
	title	= {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author	= {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina N. Toutanova},
	year	= {2018},
	URL	= {https://arxiv.org/abs/1810.04805}
}

@inproceedings{bozorgi,
	author = {Bozorgi, Mehran and Saul, Lawrence and Savage, Stefan and Voelker, Geoffrey},
	year = {2010},
	month = {06},
	pages = {105-114},
	title = {Beyond Heuristics: Learning to Classify Vulnerabilities and Predict Exploits},
	doi = {10.1145/1835804.1835821}
}

@article{vulnerwatch,
	author = {Janamian, Cook, Logan, Lim, Ulloa},
	year = {2021},
	title = {Using NLP to Predict the Severity of Cybersecurity Vulnerabilities},
	URL = {https://github.com/twlim1/VulnerWatch/blob/master/reports/Report.pdf}
}

@inproceedings {FER,
	author = {Đerek, Groš, Mikuc, Vuković},
	year = {2021},
	title = {Izvori prijetnji i prijetnje},
	course = {Cybersecurity of computer systems},
	organization = {Faculty of Electrical Engineering and Computer Science, Zagreb}
}

@inproceedings {boser,
	author = {Boser, B.E., Guyon, I.M. and Vapnik, V.N.},
	year = {1992},
	title = {A Training Algorithm for Optimal Margin Classifiers}
}

@article {smola,
	author = {Smola, A.J., Schölkopf, B.},
	year = {2004},
	title = {A tutorial on support vector regression},
	journal = {Statistics and Computing},
	volume = {14},
	pages = {199-222}
}

@Inbook{Awad2015,
	author="Awad, Mariette
	and Khanna, Rahul",
	title="Support Vector Regression",
	bookTitle="Efficient Learning Machines: Theories, Concepts, and Applications for Engineers and System Designers",
	year="2015",
	publisher="Apress",
	address="Berkeley, CA",
	pages="67--80",
	abstract="Rooted in statistical learning or Vapnik-Chervonenkis (VC) theory, support vector machines (SVMs) are well positioned to generalize on yet-to-be-seen data. The SVM concepts presented in Chapter 3can be generalized to become applicable to regression problems. As in classification, support vector regression (SVR) is characterized by the use of kernels, sparse solution, and VC control of the margin and the number of support vectors. Although less popular than SVM, SVR has been proven to be an effective tool in real-value function estimation. As a supervised-learning approach, SVR trains using a symmetrical loss function, which equally penalizes high and low misestimates. Using Vapnik's -insensitive approach, a flexible tube of minimal radius is formed symmetrically around the estimated function, such that the absolute values of errors less than a certain threshold are ignored both above and below the estimate. In this manner, points outside the tube are penalized, but those within the tube, either above or below the function, receive no penalty. One of the main advantages of SVR is that its computational complexity does not depend on the dimensionality of the input space. Additionally, it has excellent generalization capability, with high prediction accuracy.",
	isbn="978-1-4302-5990-9",
	doi="10.1007/978-1-4302-5990-9_4",
	url="https://doi.org/10.1007/978-1-4302-5990-9_4"
}

@article{WU201926,
	title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimizationb},
	journal = {Journal of Electronic Science and Technology},
	volume = {17},
	number = {1},
	pages = {26-40},
	year = {2019},
	issn = {1674-862X},
	doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
	url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
	author = {Jia Wu and Xiu-Yun Chen and Hao Zhang and Li-Dong Xiong and Hang Lei and Si-Hao Deng},
	keywords = {Bayesian optimization, Gaussian process, hyperparameter optimization, machine learning},
	abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.}
}

@misc{mikolov,
	doi = {10.48550/ARXIV.1301.3781},
	url = {https://arxiv.org/abs/1301.3781},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Efficient Estimation of Word Representations in Vector Space},
	publisher = {arXiv},
	year = {2013},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{vaswani,
	doi = {10.48550/ARXIV.1706.03762},
	url = {https://arxiv.org/abs/1706.03762},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Attention Is All You Need},
	publisher = {arXiv},
	year = {2017},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{shimanaka,
	doi = {10.48550/ARXIV.1907.12679},
	url = {https://arxiv.org/abs/1907.12679},
	author = {Shimanaka, Hiroki and Kajiwara, Tomoyuki and Komachi, Mamoru},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Machine Translation Evaluation with BERT Regressor},
	publisher = {arXiv},
	year = {2019},
	copyright = {Creative Commons Attribution 4.0 International}
}

